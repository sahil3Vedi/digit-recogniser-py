{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning on MNIST without Tensorflow",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil3Vedi/digit-recogniser-py/blob/master/Deep_Learning_on_MNIST_without_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "44tpxiU1auZB",
        "colab_type": "code",
        "outputId": "ff6fff38-7eca-420e-b403-2bbf8f83cca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#declaring all the dependencies NO KERAS OR TENSORFLOW\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "from google.colab import drive\n",
        "from math import e\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D0dOuVLGDRpF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural Network is Trained in 4 Stages:\n",
        "\n",
        "1) Network recieves input from model\n",
        "FUNCTION: NN.getInput(model.inputList)\n",
        "VARIABLES INVOLVED: NN.layers[0].inputVals\n",
        "\n",
        "2) Network activates one layer after the other.\n",
        "FUNCTION: NN.activate(activationFunction)\n",
        "VARIABLES INVOLVED: NN.layers[i].inputVals, NN.layers[i].activations, NN.layers[i].activDerivates, NN.layers[i].layerWeights, NN.layers[i].bias, NN.layers[i].biasWeights\n",
        "\n",
        "3) Model calculates cost from the target value of input data and answer provided by network.\n",
        "FUNCTION: model.getCost(NN)\n",
        "VARIABLES INVOLVED: NN.layers[-1].activations, NN.layers[-1].forwardError, model.cost\n",
        "\n",
        "4) Network readjusts the weights based on the cost and error information.\n",
        "FUNCTION: NN.backpropagate()\n",
        "VARIABLES INVOLVED: NN.layers[i].forwardError, NN.layers[i].diffWeights, NN.layers[i].diffBias, NN.layers[i-1].activations, NN.layers[i-1].forwardError"
      ]
    },
    {
      "metadata": {
        "id": "A8tGUPpNa_SB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class neuralNetwork:\n",
        "  layers=[]\n",
        "  finalError=[]\n",
        "  inputElem=0\n",
        "  hiddenElem=0\n",
        "  outputElem=0\n",
        "  \n",
        "  #When a neural network is declared \n",
        "  def __init__(self):\n",
        "    self.inputElem=784\n",
        "    self.outputElem=10\n",
        "    self.hiddenElem=20\n",
        "    \n",
        "    inputLayer = layer(self.inputElem,\"normalize\",None)\n",
        "    self.layers.append(inputLayer)\n",
        "    \n",
        "    temp = layer(self.hiddenElem,\"ReLU\",self.layers[0])\n",
        "    self.layers.append(temp)\n",
        "    temp = layer(self.hiddenElem,\"ReLU\",self.layers[1])\n",
        "    self.layers.append(temp)\n",
        "    temp = layer(self.hiddenElem,\"ReLU\",self.layers[2])\n",
        "    self.layers.append(temp)\n",
        "    \n",
        "    outputLayer = layer(self.outputElem,\"softMax\",self.layers[3])\n",
        "    self.layers.append(outputLayer)\n",
        "    #TOTAL 5 LAYERS\n",
        "    print(\"Neural Network Initialised...\")\n",
        "    \n",
        "  def getInput(self,i):\n",
        "    self.layers[0].inputVals=i\n",
        "    \n",
        "  def activate(self):\n",
        "    for i in range(0,len(self.layers)):\n",
        "      temp=self.layers[i].inputVals\n",
        "      activation=self.layers[i].activation\n",
        "      \n",
        "      if (activation==\"normalize\"):#typically used for the input layer. if you ask me it makes perfect sense to treat normalisation as input layer activation.\n",
        "        temp2=[(-1+(int(x)*2)/255) for x in temp]\n",
        "        temp3=None #this is because normalise is only being used on input layer where we dont need any back prop hence no need to calculate derivative.\n",
        "      \n",
        "      if (activation==\"ReLU\"):\n",
        "        temp2=[]\n",
        "        temp3=[]\n",
        "        for x in temp:\n",
        "          if (x>0):\n",
        "            y=x\n",
        "            temp3.append(1)\n",
        "          else:\n",
        "            y=0*x\n",
        "            temp3.append(0)\n",
        "          temp2.append(y)\n",
        "          \n",
        "      if (activation==\"tanH\"):\n",
        "        temp2=[]\n",
        "        temp3=[]\n",
        "        for x in temp:\n",
        "          a=e**(10*x)\n",
        "          b=e**(-10*x)\n",
        "          temp2.append((a-b)/(a+b))\n",
        "        for y in temp2:\n",
        "          temp3.append(y*(1-y))\n",
        "          \n",
        "      if (activation==\"softMax\"):\n",
        "        evec=[e**x for x in temp]\n",
        "        esum=np.sum(evec)\n",
        "        temp2=[y/esum for y in evec]\n",
        "        temp3=[q*(1-q) for q in temp2]\n",
        "        \n",
        "      self.layers[i].activations=temp2\n",
        "      self.layers[i].activDerivatives=temp3\n",
        "      \n",
        "      if(i!=len(self.layers)-1):#this is because we dont need to feed forward beyond the output layer.\n",
        "        self.layers[i].feedForward(self.layers[i+1])\n",
        "  \n",
        "  def backpropagate(self):\n",
        "    for i in range(len(self.layers)):\n",
        "      ind=-i-1\n",
        "      if(i!=len(self.layers)-1):\n",
        "        self.layers[ind].updateWeights(self.layers[ind-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JI26jhojzoG2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class model:\n",
        "  epoch=0\n",
        "  learningRate=0.01\n",
        "  cost=0\n",
        "  target=0\n",
        "  inputFile=[]\n",
        "  trainingEntries=0\n",
        "  accuracy=0\n",
        "  \n",
        "  def response(self,NN):\n",
        "    maxI=max(NN.layers[-1].activations)\n",
        "    indX=NN.layers[-1].activations.index(maxI)\n",
        "    if(str(self.target)==str(indX)):\n",
        "      self.accuracy+=1\n",
        "    self.epoch+=1\n",
        "    if(self.epoch%100==0):\n",
        "      print(\"Epoch: \" + str(self.epoch) + \" Cost: \" + str(self.cost) + \" Target: \" + str((self.target)) + \" Answer : \" + str(indX))\n",
        "      print(NN.layers[-3].activations)\n",
        "      print(\"Accuracy: \" + str(self.accuracy))\n",
        "      self.accuracy=0\n",
        "      \n",
        "  \n",
        "  def getInput(self,inputFile):\n",
        "    with open(inputFile) as csvFile:\n",
        "      csvReader = csv.reader(csvFile)\n",
        "      self.inputFile = list(csvReader)\n",
        "      self.trainingEntries = len(self.inputFile)-1  #n-1 because first row is header row\n",
        "      print(\"Total entries excluding header: \" + str(self.trainingEntries) + \" rows\")\n",
        "      \n",
        "  def getCost(self,NN):\n",
        "    targetVector=[]\n",
        "    for i in range(10):\n",
        "      if (str(i)==str(self.target)):\n",
        "        targetVector.append(1)\n",
        "      else:\n",
        "        targetVector.append(0)\n",
        "    output=NN.layers[-1].activations\n",
        "    error=[x-y for (x,y) in zip(output,targetVector)]\n",
        "    self.cost=0.5*(np.sum(np.square(error)))\n",
        "    NN.layers[-1].forwardError=error\n",
        "      \n",
        "  def trainNetwork(self,NN):\n",
        "    inputList=self.inputFile\n",
        "    inputList.pop(0)\n",
        "    for i in inputList:\n",
        "      self.target=i[0]\n",
        "      i.pop(0)\n",
        "      NN.getInput(i)\n",
        "      NN.activate()\n",
        "      self.getCost(NN)\n",
        "      NN.backpropagate()\n",
        "      Model.response(NN)\n",
        "      \n",
        "  def invCross(self,l1,l2): #this is the ONLY part that\n",
        "    temp=[]\n",
        "    for x in l1:\n",
        "      temptemp=[]\n",
        "      for y in l2:\n",
        "        temptemp.append(x*y)\n",
        "      temp.append(temptemp)\n",
        "    return temp\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aiSyxuUxdsDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class layer:\n",
        "  numberElements=0\n",
        "  bias=0\n",
        "  activation=\"default\"\n",
        "  inputVals=[]\n",
        "  activations=[]\n",
        "  activDerivatives=[]\n",
        "  forwardError=[]\n",
        "  layerWeights=[]\n",
        "  biasWeights=[]\n",
        "  \n",
        "  #When a layer is declared\n",
        "  def __init__(self,no_of_elements,activation,previousLayer):\n",
        "    self.numberElements=no_of_elements\n",
        "    self.bias=0.0001\n",
        "    self.activation=activation\n",
        "    #Initialising the Weight Matrix\n",
        "    temp=[]\n",
        "    if(previousLayer!=None):\n",
        "      for i in range(previousLayer.numberElements):\n",
        "        temptemp=[]\n",
        "        for j in range(self.numberElements):\n",
        "          temptemp.append((np.random.random_sample()-0.5)*0.1)\n",
        "        temp.append(temptemp)\n",
        "    self.layerWeights=temp\n",
        "    #Initialising the Bias Matrix\n",
        "    self.biasWeights=[]\n",
        "    for i in range(self.numberElements):\n",
        "      self.biasWeights.append(np.random.random_sample()-0.5)\n",
        "      \n",
        "  def feedForward(self,nextLayer):\n",
        "    tempbias=self.bias\n",
        "    biasVec=nextLayer.biasWeights\n",
        "    temp2=np.matmul(self.activations,nextLayer.layerWeights)\n",
        "    temp3=np.multiply(tempbias,biasVec)\n",
        "    nextLayer.inputVals=[x+y for (x,y) in zip(temp2,temp3)]\n",
        "    \n",
        "  def updateWeights(self,prevLayer):\n",
        "    #declaring the Differential diagonal matrix\n",
        "    dmx=[]\n",
        "    for i in range(self.numberElements):\n",
        "      dtemp=[]\n",
        "      oi=self.activations[i]\n",
        "      for j in range(self.numberElements):\n",
        "        if(i==j):\n",
        "          dtemp.append(oi*(1-oi))\n",
        "        else:\n",
        "          dtemp.append(0)\n",
        "      dmx.append(dtemp)\n",
        "    #declaring do vector\n",
        "    dovec=np.matmul(dmx,self.forwardError)\n",
        "    wmx=Model.invCross(dovec,prevLayer.activations)\n",
        "    bvec=np.multiply((self.bias*Model.learningRate),dovec)\n",
        "    wMx=np.multiply(Model.learningRate,wmx)\n",
        "    tempW=np.subtract(self.layerWeights,np.transpose(wMx))\n",
        "    tempB=np.subtract(self.biasWeights,bvec)\n",
        "    self.layerWeights=tempW\n",
        "    prevLayer.forwardError=np.matmul(self.layerWeights,dovec)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GihhafanX14r",
        "colab_type": "code",
        "outputId": "20f9457a-8c73-4048-e189-8a71f282dfcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "cell_type": "code",
      "source": [
        "#-----MAIN FUNCTION-----\n",
        "\n",
        "Model = model()\n",
        "inputFile='drive/My Drive/MNIST Image CSV/train.csv'\n",
        "Model.getInput(inputFile)\n",
        "\n",
        "NN = neuralNetwork()\n",
        "Model.trainNetwork(NN)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total entries excluding header: 42000 rows\n",
            "Neural Network Initialised...\n",
            "Epoch: 100 Cost: 0.44988206989647844 Target: 5 Answer : 5\n",
            "[0.04162331634693489, 0.004349938908060477, 0.05985966012014617, -0.0, 0.13236807886870583, -0.0, -0.0, 0.07296243561458628, -0.0, 0.12821371087230182, -0.0, 0.06404642696149315, -0.0, -0.0, 0.16453442611710098, -0.0, 0.026432219610516002, -0.0, 0.13790272397274136, -0.0]\n",
            "Accuracy: 10\n",
            "Epoch: 200 Cost: 0.4499034884861538 Target: 2 Answer : 0\n",
            "[-0.0, 0.06378932496372923, 0.10192260768550206, -0.0, 0.0446389161495405, -0.0, 0.08140594161470212, 0.08387005646979227, -0.0, 0.1366144024230028, -0.0, 5.6877690550490316e-05, 0.06672204923494585, -0.0, 0.1301236805707163, -0.0, -0.0, -0.0, 0.0232541764466844, 0.0027066193043644373]\n",
            "Accuracy: 6\n",
            "Epoch: 300 Cost: 0.4500767242767653 Target: 4 Answer : 0\n",
            "[-0.0, 0.05820499053710643, 0.08897093629929934, -0.0, 0.0357031597490372, -0.0, 0.051506939837320256, 0.07069479318111825, -0.0, 0.11024003379278317, -0.0, 0.00023464225753826515, 0.08649087407925193, -0.0, 0.1365186740544093, -0.0, -0.0, -0.0, 0.053973238809040036, 0.016073909619397326]\n",
            "Accuracy: 15\n",
            "Epoch: 400 Cost: 0.4499986622029466 Target: 6 Answer : 2\n",
            "[-0.0, 0.029347611040720447, 0.028392036142888627, -0.0, 0.060979190629545606, 0.012857491216589538, 0.006325525071571439, 0.017328348088858966, -0.0, 0.07700065823633856, 0.024357809643410534, 0.008906518380530009, 0.008301525874383663, -0.0, 0.07434365901491101, -0.0, -0.0, -0.0, 0.03719809902876945, -0.0]\n",
            "Accuracy: 8\n",
            "Epoch: 500 Cost: 0.45002753209224244 Target: 9 Answer : 0\n",
            "[-0.0, -0.0, 0.11805502598659212, 0.016032260432954613, 0.10900107247077606, -0.0, 0.005768969763113144, 0.12859586494824868, -0.0, 0.13289271064094157, -0.0, 0.00878230903128215, -0.0, -0.0, 0.1798599484361603, -0.0, -0.0, -0.0, 0.1097221479257288, -0.0]\n",
            "Accuracy: 8\n",
            "Epoch: 600 Cost: 0.4498896053544508 Target: 2 Answer : 0\n",
            "[-0.0, 0.06886280947511128, 0.12078775197802556, -0.0, 0.015837717941124145, -0.0, 0.11950481487213893, 0.11932047725374262, -0.0, 0.15075954052711876, -0.0, -0.0, 0.07854644502507019, -0.0, 0.1669464704999735, -0.0, -0.0, -0.0, 0.0550141186170565, 0.04653364706111621]\n",
            "Accuracy: 8\n",
            "Epoch: 700 Cost: 0.45000875243445554 Target: 6 Answer : 2\n",
            "[-0.0, -0.0, 0.07525684408108818, -0.0, 0.05931805994188813, -0.0, 0.02578061613981812, 0.027457087025035437, 0.00706601524760874, 0.13954788317088987, -0.0, 0.0029788103549479045, -0.0, -0.0, 0.10158761522597788, -0.0, -0.0, -0.0, 0.07968774164872944, -0.0]\n",
            "Accuracy: 9\n",
            "Epoch: 800 Cost: 0.4500194474948804 Target: 9 Answer : 2\n",
            "[-0.0, -0.0, 0.08102596189271442, 0.013848183063932857, 0.11169502536193075, -0.0, -0.0, 0.11266533192653129, -0.0, 0.14099198764587087, -0.0, 0.03827501393404188, -0.0, -0.0, 0.15937829870882378, -0.0, -0.0, -0.0, 0.11628614860034774, -0.0]\n",
            "Accuracy: 12\n",
            "Epoch: 900 Cost: 0.4500485652042004 Target: 4 Answer : 0\n",
            "[-0.0, 0.042725349521331245, 0.05679232475034945, -0.0, 0.05180947670717628, -0.0, 0.05128871698959718, 0.08757791758292033, -0.0, 0.10589254531007615, -0.0, 0.005358693520926562, 0.05190701419311826, -0.0, 0.11990905169532914, -0.0, -0.0, -0.0, 0.03632162987535219, 0.01899759226797083]\n",
            "Accuracy: 6\n",
            "Epoch: 1000 Cost: 0.45010379133003736 Target: 4 Answer : 0\n",
            "[-0.0, 0.05718729708370356, 0.10549879805551646, -0.0, 0.062218036832728106, -0.0, 0.04886753544583211, 0.11203759473041947, -0.0, 0.1164104261672827, -0.0, -0.0, 0.09274767432681352, -0.0, 0.18368545907191333, -0.0, -0.0, -0.0, 0.09657494620576193, -0.0]\n",
            "Accuracy: 8\n",
            "Epoch: 1100 Cost: 0.4501143521064 Target: 4 Answer : 0\n",
            "[-0.0, 0.044757304619211374, 0.05102344123440577, -0.0, 0.13439286523623928, -0.0, -0.0, 0.060316609597000116, -0.0, 0.11466630076763668, -0.0, -0.0, 0.0703716949549953, -0.0, 0.18228702542535416, -0.0, -0.0, -0.0, 0.14450286443574267, -0.0]\n",
            "Accuracy: 6\n",
            "Epoch: 1200 Cost: 0.4499489818480622 Target: 5 Answer : 0\n",
            "[-0.0, 0.02487383516467408, 0.07420362050061456, -0.0, 0.06209230007925648, -0.0, 0.00010745777369416712, 0.11583938012785756, -0.0, 0.1014785554333524, -0.0, 0.02237108955981241, 0.022974050094188847, -0.0, 0.16104624503219622, -0.0, -0.0, -0.0, 0.12051867748874091, -0.0]\n",
            "Accuracy: 6\n",
            "Epoch: 1300 Cost: 0.4500267469236858 Target: 8 Answer : 2\n",
            "[-0.0, -0.0, 0.13142248471298662, -0.0, 0.0938089000373233, -0.0, 0.04616943754166416, 0.07715739173632616, -0.0, 0.20954955508272696, -0.0, -0.0, -0.0, -0.0, 0.1761989529762411, -0.0, -0.0, -0.0, 0.10703506770722342, -0.0]\n",
            "Accuracy: 7\n",
            "Epoch: 1400 Cost: 0.44999498322078185 Target: 6 Answer : 0\n",
            "[-0.0, 0.040116940473942375, 0.028544826646151353, -0.0, 0.0550610155990213, -0.0, 0.008928217298395156, 0.04489878858384882, -0.0, 0.09510831552489309, -0.0, 0.01193907671834942, 0.02067484835767682, -0.0, 0.09413661605646323, -0.0, -0.0, -0.0, 0.039572613445904056, -0.0]\n",
            "Accuracy: 6\n",
            "Epoch: 1500 Cost: 0.4498868064450864 Target: 2 Answer : 0\n",
            "[-0.0, 0.05482303015679703, 0.09890989703173819, -0.0, 0.07705006171984485, -0.0, 0.04877840160527463, 0.044046091512946174, -0.0, 0.18105691844204413, -0.0, 0.0016281797704389735, 0.051380454709106904, -0.0, 0.12776234695837133, -0.0, -0.0, -0.0, 0.04739001085806495, -0.0]\n",
            "Accuracy: 11\n",
            "Epoch: 1600 Cost: 0.4500169138860652 Target: 9 Answer : 0\n",
            "[-0.0, 0.041870420067145475, 0.07961609786314643, -0.0, 0.032173125522056735, -0.0, 0.05115765024078237, 0.1135460487827211, -0.0, 0.1338317296390196, -0.0, -0.0, 0.07998916022402172, -0.0, 0.15573415957406034, -0.0, -0.0, -0.0, 0.10522856360331621, 0.031033534766268276]\n",
            "Accuracy: 11\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}