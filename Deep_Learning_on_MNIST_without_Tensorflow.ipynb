{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning on MNIST without Tensorflow",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil3Vedi/digit-recogniser-py/blob/master/Deep_Learning_on_MNIST_without_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "44tpxiU1auZB",
        "colab_type": "code",
        "outputId": "23d6265e-d3bf-418f-c10d-72efd4f1319f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#declaring all the dependencies NO KERAS OR TENSORFLOW\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D0dOuVLGDRpF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural Network is Trained in 4 Stages:\n",
        "\n",
        "1) Network recieves input from model\n",
        "FUNCTION: NN.getInput(model.inputList)\n",
        "VARIABLES INVOLVED: NN.layers[0].inputVals\n",
        "\n",
        "2) Network activates one layer after the other.\n",
        "FUNCTION: NN.activate(activationFunction)\n",
        "VARIABLES INVOLVED: NN.layers[i].inputVals, NN.layers[i].activations, NN.layers[i].activDerivates, NN.layers[i].layerWeights, NN.layers[i].bias, NN.layers[i].biasWeights\n",
        "\n",
        "3) Model calculates cost from the target value of input data and answer provided by network.\n",
        "FUNCTION: model.getCost(NN)\n",
        "VARIABLES INVOLVED: NN.layers[-1].activations, NN.layers[-1].forwardError, model.cost\n",
        "\n",
        "4) Network readjusts the weights based on the cost and error information.\n",
        "FUNCTION: NN.backpropagate()\n",
        "VARIABLES INVOLVED: NN.layers[i].forwardError, NN.layers[i].diffWeights, NN.layers[i].diffBias, NN.layers[i-1].activations, NN.layers[i-1].forwardError"
      ]
    },
    {
      "metadata": {
        "id": "A8tGUPpNa_SB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class neuralNetwork:\n",
        "  layers=[]\n",
        "  finalError=[]\n",
        "  inputElem=0\n",
        "  hiddenElem=0\n",
        "  outputElem=0\n",
        "  \n",
        "  #When a neural network is declared \n",
        "  def __init__(self):\n",
        "    self.inputElem=784\n",
        "    self.outputElem=10\n",
        "    self.hiddenElem=50\n",
        "    \n",
        "    inputLayer = layer(self.inputElem,\"normalize\",None)\n",
        "    self.layers.append(inputLayer)\n",
        "    \n",
        "    temp = layer(self.hiddenElem,\"ReLU\",self.layers[0])\n",
        "    self.layers.append(temp)\n",
        "    temp = layer(self.hiddenElem,\"ReLU\",self.layers[1])\n",
        "    self.layers.append(temp)\n",
        "    temp = layer(self.hiddenElem,\"ReLU\",self.layers[2])\n",
        "    self.layers.append(temp)\n",
        "    \n",
        "    outputLayer = layer(self.outputElem,\"softMax\",self.layers[3])\n",
        "    self.layers.append(outputLayer)\n",
        "    #TOTAL 5 LAYERS\n",
        "    \n",
        "    print(\"Neural Network Initialised..\")\n",
        "    print(\"Number of layers: \" + str(len(self.layers)))\n",
        "    print(\"Size of Weight Mx at Input: \" + str(np.shape(self.layers[0].layerWeights)))\n",
        "    print(\"Size of Weight Mx at Hidden: \" + str(np.shape(self.layers[1].layerWeights)))\n",
        "    print(\"Size of Weight Mx at Hidden: \" + str(np.shape(self.layers[2].layerWeights)))\n",
        "    print(\"Size of Weight Mx at Hidden: \" + str(np.shape(self.layers[3].layerWeights)))\n",
        "    print(\"Size of Weight Mx at Output: \" + str(np.shape(self.layers[4].layerWeights)))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JI26jhojzoG2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class model:\n",
        "  cost=0\n",
        "  inputFile=[]\n",
        "  trainingEntries=0\n",
        "  \n",
        "  def getInput(self,inputFile):\n",
        "    with open(inputFile) as csvFile:\n",
        "      csvReader = csv.reader(csvFile)\n",
        "      self.inputFile = list(csvReader)\n",
        "      self.trainingEntries = len(self.inputFile)-1  #n-1 because first row is header row\n",
        "      print(\"Total entries excluding header: \" + str(self.trainingEntries) + \" rows\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aiSyxuUxdsDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class layer:\n",
        "  numberElements=0\n",
        "  bias=0\n",
        "  activation=\"default\"\n",
        "  inputVals=[]\n",
        "  activations=[]\n",
        "  activDerivatives=[]\n",
        "  forwardError=[]\n",
        "  layerWeights=[]\n",
        "  biasWeights=[]\n",
        "  \n",
        "  #When a layer is declared\n",
        "  def __init__(self,no_of_elements,activation,previousLayer):\n",
        "    self.numberElements=no_of_elements\n",
        "    self.bias=0.5\n",
        "    self.activation=activation\n",
        "    #Initialising the Weight Matrix\n",
        "    temp=[]\n",
        "    if(previousLayer!=None):\n",
        "      for i in range(previousLayer.numberElements):\n",
        "        temptemp=[]\n",
        "        for j in range(self.numberElements):\n",
        "          temptemp.append(np.random.random_sample()-0.5)\n",
        "        temp.append(temptemp)\n",
        "    self.layerWeights=temp\n",
        "    #Initialising the Bias Matrix\n",
        "    for i in range(self.numberElements):\n",
        "      self.biasWeights.append(np.random.random_sample()-0.5)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GihhafanX14r",
        "colab_type": "code",
        "outputId": "93e2bc48-6f74-4fd6-c1d9-663a95f0b453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "#-----MAIN FUNCTION-----\n",
        "\n",
        "Model = model()\n",
        "inputFile='drive/My Drive/MNIST Image CSV/train.csv'\n",
        "Model.getInput(inputFile)\n",
        "\n",
        "NN = neuralNetwork()\n",
        "#Model.trainNetwork(NN)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total entries excluding header: 42000 rows\n",
            "Neural Network Initialised..\n",
            "Number of layers: 5\n",
            "Size of Weight Mx at Input: (0,)\n",
            "Size of Weight Mx at Hidden: (784, 50)\n",
            "Size of Weight Mx at Hidden: (50, 50)\n",
            "Size of Weight Mx at Hidden: (50, 50)\n",
            "Size of Weight Mx at Output: (50, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}